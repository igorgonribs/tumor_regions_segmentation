{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using the ORCA dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 18:11:58,758 :: INFO load_dataset :: [training] ../../datasets/BONE_CHANNELS_reduced_size/training\n",
      "2023-04-22 18:11:58,761 :: INFO load_dataset :: [training] ../../datasets/BONE_CHANNELS_reduced_size/training\n",
      "2023-04-22 18:11:58,763 :: INFO load_dataset :: [testing] ../../datasets/BONE_CHANNELS_reduced_size/testing\n",
      "2023-04-22 18:11:58,764 :: INFO create_dataloader :: Train images (640x640): 0 augmentation: random\n",
      "2023-04-22 18:11:58,765 :: INFO create_dataloader :: Test images (640x640): 0 augmentation: no_augmentation\n",
      "2023-04-22 18:11:58,766 :: INFO train_model_with_validation :: Runing on: cpu | GPU available? False\n",
      "2023-04-22 18:11:59,643 :: INFO train_model_with_validation :: \n",
      "2023-04-22 18:11:59,644 :: INFO train_model_with_validation :: --------------------\n",
      "2023-04-22 18:11:59,646 :: INFO train_model_with_validation :: Epoch 2/3 random_9_operations (0m 0s) 2023-04-22 18:11:59.646446\n",
      "2023-04-22 18:11:59,646 :: INFO train_model_with_validation :: --------------------\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m     50\u001b[0m result_file_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_to_use \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/training/bones_training_accuracy_loss.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mtrain_model_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maugmentation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maugmentation_operations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mresult_file_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_file_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Igor\\dali\\tumor_regions_segmentation-main\\sourcecode\\BONE_CHANNELS\\bones_train_512x512.py:115\u001b[0m, in \u001b[0;36mtrain_model_with_validation\u001b[1;34m(dataloaders, model, patch_size, n_epochs, start_epoch, batch_size, use_cuda, output_dir, augmentation_strategy, augmentation_operations, result_file_csv)\u001b[0m\n\u001b[0;32m    111\u001b[0m             running_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n\u001b[0;32m    113\u001b[0m             qtd_images \u001b[38;5;241m=\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m qtd_images\n\u001b[1;32m--> 115\u001b[0m     epoch_loss[phase] \u001b[38;5;241m=\u001b[39m \u001b[43mrunning_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     epoch_acc[phase] \u001b[38;5;241m=\u001b[39m running_accuracy \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders[phase]\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# save the model - each epoch\u001b[39;00m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "dataset_to_use = \"BONE_CHANNELS_reduced_size\"\n",
    "dataset_dir = \"../../datasets/\" + dataset_to_use\n",
    "model_dir = \"../../models\"\n",
    "start_epoch = 1\n",
    "n_epochs = 3\n",
    "\n",
    "if dataset_to_use == \"BONE_CHANNELS_reduced_size\":\n",
    "    from bones_train_512x512 import *\n",
    "else:\n",
    "    from bones_train import *\n",
    "    \n",
    "augmentation_strategy = \"random\" # \"no_augmentation\", \"color_augmentation\", \"inpainting_augmentation\", \"standard\", \"random\"\n",
    "augmentation = [None,\n",
    "                \"horizontal_flip\", \n",
    "                \"vertical_flip\", \n",
    "                \"rotation\", \n",
    "                \"transpose\", \n",
    "                \"elastic_transformation\", \n",
    "                \"grid_distortion\", \n",
    "                \"optical_distortion\", \n",
    "                \"color_transfer\", \n",
    "                \"inpainting\"]\n",
    "#[None, \"horizontal_flip\", \"vertical_flip\", \"rotation\", \"transpose\", \"elastic_transformation\", \"grid_distortion\", \"optical_distortion\", \"color_transfer\", \"inpainting\"]\n",
    "\n",
    "start_epoch = 1\n",
    "batch_size = 1\n",
    "patch_size = (640, 640)\n",
    "color_model = \"LAB\"\n",
    "dataloaders = create_dataloader(tile_size=\"{}x{}\".format(patch_size[0], patch_size[1]),\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                img_input_size=patch_size,\n",
    "                                img_output_size=patch_size,\n",
    "                                dataset_dir=dataset_dir,\n",
    "                                color_model=color_model,\n",
    "                                augmentation=augmentation,\n",
    "                                augmentation_strategy=augmentation_strategy,\n",
    "                                start_epoch=start_epoch,\n",
    "                                validation_split=0.0)\n",
    "\n",
    "# loads our u-net based model to continue previous training\n",
    "#trained_model_version = \"ORCA__Size-640x640_Epoch-10_Images-4181_Batch-1__random_distortion\"\n",
    "#trained_model_path = \"{}/{}.pth\".format(model_dir, trained_model_version)\n",
    "#model = load_checkpoint(file_path=trained_model_path, img_input_size=patch_size, use_cuda=True)\n",
    "\n",
    "# starts the training from scratch\n",
    "model = None\n",
    "\n",
    "# train the model\n",
    "result_file_csv = \"../../datasets/\" + dataset_to_use + \"/training/bones_training_accuracy_loss.csv\"\n",
    "train_model_with_validation(dataloaders=dataloaders,\n",
    "                            model=model,\n",
    "                            n_epochs=n_epochs,\n",
    "                            start_epoch=start_epoch,\n",
    "                            augmentation_strategy=augmentation_strategy,\n",
    "                            output_dir=model_dir,\n",
    "                            augmentation_operations=augmentation,\n",
    "                            result_file_csv=result_file_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_random.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "#train = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "#valid = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train = df[df['phase'] == 'train']\n",
    "valid = df[df['phase'] == 'valid']\n",
    "\n",
    "\n",
    "#train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "#valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "#display(HTML(train.tail(10).to_html()))\n",
    "#display(HTML(valid.tail(10).to_html()))\n",
    "#print(\"Train: {} / Valid: {}\".format(train['accuracy'].max(), valid['accuracy'].max()))\n",
    "\n",
    "train_best_accuracy = train[train['accuracy']==train['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "\n",
    "valid_best_accuracy = valid[valid['accuracy']==valid['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (random)': train['accuracy'].to_numpy(),\n",
    "    'accuracy valid (random)': valid['accuracy'].to_numpy()},\n",
    "    #'accuracy train (one_by_epoch)': train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    #'accuracy valid (one_by_epoch)': valid_one_by_epoch['accuracy'].to_numpy()}, \n",
    "    index = train['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.6,1))#subplots=True)\n",
    "\n",
    "# Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.\n",
    "#df_loss_lines = pd.DataFrame({\n",
    "#    'loss train': train['loss'].to_numpy(),\n",
    "#    'loss valid': valid['loss'].to_numpy()}, \n",
    "#    index = train['epoch'])\n",
    "#loss_lines = df_loss_lines.plot.line(ylim=(0,1))#subplots=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (no augmentation) {:.3f}'.format(train_no_augmentation['accuracy'].mean()): train_no_augmentation['accuracy'].to_numpy(),\n",
    "    'accuracy valid (no augmentation) {:.3f}'.format(valid_no_augmentation['accuracy'].mean()): valid_no_augmentation['accuracy'].to_numpy(),\n",
    "    'accuracy train (one operation by epoch) {:.3f}'.format(train_one_by_epoch['accuracy'].mean()): train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch) {:.3f}'.format(valid_one_by_epoch['accuracy'].mean()): valid_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy train (random composition) {:.3f}'.format(train_random['accuracy'].mean()): train_random['accuracy'].to_numpy(),\n",
    "    'accuracy valid (random composition) {:.3f}'.format(valid_random['accuracy'].mean()): valid_random['accuracy'].to_numpy()}, \n",
    "    index = train_one_by_epoch['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_200.png\".format(training_dir))\n",
    "    \n",
    "print(\"'No augmentation' mean accuracy: {}\".format(valid_no_augmentation['accuracy'].mean()))\n",
    "train_best_accuracy = train_no_augmentation[train_no_augmentation['accuracy']==train_no_augmentation['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_no_augmentation[valid_no_augmentation['accuracy']==valid_no_augmentation['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "print(\"'One operation by epoch' mean accuracy: {}\".format(valid_one_by_epoch['accuracy'].mean()))\n",
    "train_best_accuracy = train_one_by_epoch[train_one_by_epoch['accuracy']==train_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_one_by_epoch[valid_one_by_epoch['accuracy']==valid_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "print(\"'Random composition' mean accuracy: {}\".format(valid_random['accuracy'].mean()))\n",
    "train_best_accuracy = train_random[train_random['accuracy']==train_random['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_random[valid_random['accuracy']==valid_random['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'loss train (no augmentation)': train_no_augmentation['loss'].to_numpy(),\n",
    "    'loss valid (no augmentation)': valid_no_augmentation['loss'].to_numpy(),\n",
    "    'loss train (one operation by epoch)': train_one_by_epoch['loss'].to_numpy(),\n",
    "    'loss valid (one operation by epoch)': valid_one_by_epoch['loss'].to_numpy(),\n",
    "    'loss train (random composition)': train_random['loss'].to_numpy(),\n",
    "    'loss valid (random composition)': valid_random['loss'].to_numpy()}, \n",
    "    index = train_one_by_epoch['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0,1), figsize=(15,10), grid=True, xticks=range(0,100,8))#subplots=True)\n",
    "\n",
    "print(\"'No augmentation' mean loss: {}\".format(valid_no_augmentation['loss'].mean()))\n",
    "train_best_accuracy = train_no_augmentation[train_no_augmentation['loss']==train_no_augmentation['loss'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_no_augmentation[valid_no_augmentation['loss']==valid_no_augmentation['loss'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "print(\"'One operation by epoch' mean loss: {}\".format(valid_one_by_epoch['loss'].mean()))\n",
    "train_best_loss = train_one_by_epoch[train_one_by_epoch['loss']==train_one_by_epoch['loss'].min()]\n",
    "display(HTML(train_best_loss.to_html()))\n",
    "valid_best_loss = valid_one_by_epoch[valid_one_by_epoch['loss']==valid_one_by_epoch['loss'].min()]\n",
    "display(HTML(valid_best_loss.to_html()))\n",
    "\n",
    "print(\"'Random composition' mean loss: {}\".format(valid_random['loss'].mean()))\n",
    "train_best_loss = train_random[train_random['loss']==train_random['loss'].min()]\n",
    "display(HTML(train_best_loss.to_html()))\n",
    "valid_best_loss = valid_random[valid_random['loss']==valid_random['loss'].min()]\n",
    "display(HTML(valid_best_loss.to_html()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(HTML(train.head(30).to_html()))\n",
    "display(HTML(valid.head(30).to_html()))\n",
    "print(\"Train: {} / Valid: {}\".format(train['accuracy'].max(), valid['accuracy'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA_512x512/training\"\n",
    "csv_file_path = \"{}/training_loss_512x512.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (no augmentation)': train_no_augmentation['accuracy'].to_numpy(),\n",
    "    'accuracy valid (no augmentation)': valid_no_augmentation['accuracy'].to_numpy(),\n",
    "    'accuracy train (one operation by epoch)': train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch)': valid_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy train (random composition)': train_random['accuracy'].to_numpy(),\n",
    "    'accuracy valid (random composition)': valid_random['accuracy'].to_numpy()}, \n",
    "    index = train_one_by_epoch['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.8,1), figsize=(15,10), grid=True, xticks=range(0,500,32))#subplots=True)\n",
    "#accuracy_lines.get_figure().savefig(\"{}/plot.png\".format(training_dir))\n",
    "\n",
    "print(\"'No augmentation' mean accuracy: {}\".format(valid_no_augmentation['accuracy'].mean()))\n",
    "train_best_accuracy = train_no_augmentation[train_no_augmentation['accuracy']==train_no_augmentation['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_no_augmentation[valid_no_augmentation['accuracy']==valid_no_augmentation['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "print(\"'One operation by epoch' mean accuracy: {}\".format(valid_one_by_epoch['accuracy'].mean()))\n",
    "train_best_accuracy = train_one_by_epoch[train_one_by_epoch['accuracy']==train_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_one_by_epoch[valid_one_by_epoch['accuracy']==valid_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "print(\"'Random composition' mean accuracy: {}\".format(valid_random['accuracy'].mean()))\n",
    "train_best_accuracy = train_random[train_random['accuracy']==train_random['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_random[valid_random['accuracy']==valid_random['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (no augmentation) {:.3f}'.format(train_no_augmentation['accuracy'].mean()): train_no_augmentation['accuracy'].to_numpy(),\n",
    "    'accuracy valid (no augmentation) {:.3f}'.format(valid_no_augmentation['accuracy'].mean()): valid_no_augmentation['accuracy'].to_numpy()}, \n",
    "    index = train_no_augmentation['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_no_augmentation_200.png\".format(training_dir))\n",
    "    \n",
    "print(\"'No augmentation' mean accuracy: {}\".format(valid_no_augmentation['accuracy'].mean()))\n",
    "train_best_accuracy = train_no_augmentation[train_no_augmentation['accuracy']==train_no_augmentation['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_no_augmentation[valid_no_augmentation['accuracy']==valid_no_augmentation['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (random composition) {:.3f}'.format(train_random['accuracy'].mean()): train_random['accuracy'].to_numpy(),\n",
    "    'accuracy valid (random composition) {:.3f}'.format(valid_random['accuracy'].mean()): valid_random['accuracy'].to_numpy()}, \n",
    "    index = train_random['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_random_200.png\".format(training_dir))\n",
    "    \n",
    "\n",
    "print(\"'Random composition' mean accuracy: {}\".format(valid_random['accuracy'].mean()))\n",
    "train_best_accuracy = train_random[train_random['accuracy']==train_random['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_random[valid_random['accuracy']==valid_random['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (one operation by epoch) {:.3f}'.format(train_one_by_epoch['accuracy'].mean()): train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch) {:.3f}'.format(valid_one_by_epoch['accuracy'].mean()): valid_one_by_epoch['accuracy'].to_numpy()}, \n",
    "    index = train_one_by_epoch['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_one_by_epoch_200.png\".format(training_dir))\n",
    "\n",
    "print(\"'One operation by epoch' mean accuracy: {}\".format(valid_one_by_epoch['accuracy'].mean()))\n",
    "train_best_accuracy = train_one_by_epoch[train_one_by_epoch['accuracy']==train_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_one_by_epoch[valid_one_by_epoch['accuracy']==valid_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_by_epoch_4_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch_4_operations')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch_4_operations')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (one operation by epoch - 4) {:.3f}'.format(train_one_by_epoch['accuracy'].mean()): train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch - 4) {:.3f}'.format(valid_one_by_epoch['accuracy'].mean()): valid_one_by_epoch['accuracy'].to_numpy()}, \n",
    "    index = train_one_by_epoch['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_one_by_epoch_4_operations_200.png\".format(training_dir))\n",
    "\n",
    "print(\"'One operation by epoch' mean accuracy: {}\".format(valid_one_by_epoch['accuracy'].mean()))\n",
    "train_best_accuracy = train_one_by_epoch[train_one_by_epoch['accuracy']==train_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_one_by_epoch[valid_one_by_epoch['accuracy']==valid_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "training_dir = \"../../datasets/ORCA/training\"\n",
    "csv_file_path = \"{}/training_loss_teste.csv\".format(training_dir)\n",
    "df = pd.read_csv(csv_file_path) \n",
    "\n",
    "train_no_augmentation = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'no_augmentation')]\n",
    "valid_no_augmentation = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'no_augmentation')]\n",
    "\n",
    "train_random = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'random')]\n",
    "valid_random = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'random')]\n",
    "\n",
    "\n",
    "train_one_by_epoch = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch')]\n",
    "valid_one_by_epoch = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch')]\n",
    "\n",
    "train_one_by_epoch_4 = df.loc[(df['phase'] == 'train') & (df['augmentation'] == 'one_by_epoch_4_operations')]\n",
    "valid_one_by_epoch_4 = df.loc[(df['phase'] == 'valid') & (df['augmentation'] == 'one_by_epoch_4_operations')]\n",
    "\n",
    "# Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.\n",
    "df_accuracy_lines = pd.DataFrame({\n",
    "    'accuracy train (one operation by epoch - 7) {:.3f}'.format(train_one_by_epoch['accuracy'].mean()): train_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch - 7) {:.3f}'.format(valid_one_by_epoch['accuracy'].mean()): valid_one_by_epoch['accuracy'].to_numpy(),\n",
    "    'accuracy train (one operation by epoch - 4) {:.3f}'.format(train_one_by_epoch_4['accuracy'].mean()): train_one_by_epoch_4['accuracy'].to_numpy(),\n",
    "    'accuracy valid (one operation by epoch - 4) {:.3f}'.format(valid_one_by_epoch_4['accuracy'].mean()): valid_one_by_epoch_4['accuracy'].to_numpy()}, \n",
    "    index = train_one_by_epoch_4['epoch'])\n",
    "accuracy_lines = df_accuracy_lines.plot.line(ylim=(0.65,1), figsize=(15,10), grid=True, xticks=range(0,201,8))#subplots=True)\n",
    "accuracy_lines.get_figure().savefig(\"{}/plot_one_by_epoch_200_4_and_7.png\".format(training_dir))\n",
    "\n",
    "print(\"'One operation by epoch' mean accuracy: {}\".format(valid_one_by_epoch['accuracy'].mean()))\n",
    "train_best_accuracy = train_one_by_epoch[train_one_by_epoch['accuracy']==train_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(train_best_accuracy.to_html()))\n",
    "valid_best_accuracy = valid_one_by_epoch[valid_one_by_epoch['accuracy']==valid_one_by_epoch['accuracy'].max()]\n",
    "display(HTML(valid_best_accuracy.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
